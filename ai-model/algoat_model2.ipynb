{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "# Load spacy model\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('qa_dataset.csv')\n",
    "\n",
    "# Split dataset into train and validation sets\n",
    "train_df, val_df = train_test_split(df, test_size=0.2)\n",
    "\n",
    "# Save the split datasets\n",
    "train_df.to_csv('train_qa_dataset.csv', index=False)\n",
    "val_df.to_csv('val_qa_dataset.csv', index=False)\n",
    "\n",
    "# Tokenizer and vocabulary\n",
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "def tokenize(text):\n",
    "    return tokenizer(text)\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenize(text)\n",
    "\n",
    "def build_vocab(data):\n",
    "    vocab = build_vocab_from_iterator(yield_tokens(data))\n",
    "    # Add special tokens\n",
    "    specials = [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "    vocab.set_specials(specials)\n",
    "    return vocab\n",
    "\n",
    "# Define Dataset class\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, dataframe, vocab):\n",
    "        self.dataframe = dataframe\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        question = tokenize(row['question'])\n",
    "        answer = tokenize(row['answer'])\n",
    "        question_tensor = torch.tensor([self.vocab[token] for token in question], dtype=torch.long)\n",
    "        answer_tensor = torch.tensor([self.vocab[token] for token in answer], dtype=torch.long)\n",
    "        return question_tensor, answer_tensor\n",
    "\n",
    "# Build vocabularies\n",
    "train_questions = df['question'].tolist()\n",
    "train_answers = df['answer'].tolist()\n",
    "vocab = build_vocab(train_questions + train_answers)\n",
    "\n",
    "# Add special tokens\n",
    "specials = [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "for special in specials:\n",
    "    if special not in vocab:\n",
    "        vocab.insert_token(special, len(vocab))\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = QADataset(train_df, vocab)\n",
    "val_dataset = QADataset(val_df, vocab)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=lambda x: zip(*x))\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, collate_fn=lambda x: zip(*x))\n",
    "\n",
    "# Define the Attention mechanism\n",
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        timestep = encoder_outputs.size(0)\n",
    "        h = hidden.repeat(timestep, 1, 1).transpose(0, 1)\n",
    "        encoder_outputs = encoder_outputs.transpose(0, 1)\n",
    "        attn_energies = self.score(h, encoder_outputs)\n",
    "        return torch.softmax(attn_energies, dim=1).unsqueeze(1)\n",
    "\n",
    "    def score(self, hidden, encoder_outputs):\n",
    "        energy = torch.tanh(self.attn(torch.cat([hidden, encoder_outputs], 2)))\n",
    "        energy = energy.transpose(2, 1)\n",
    "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)\n",
    "        energy = torch.bmm(v, energy)\n",
    "        return energy.squeeze(1)\n",
    "\n",
    "# Define Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, src):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "# Define Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hidden_dim, n_layers, dropout, attention):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(hidden_dim + emb_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        a = self.attention(hidden[-1], encoder_outputs)\n",
    "        a = a.permute(1, 0, 2)\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
    "        output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        prediction = self.fc_out(torch.cat((output, weighted), dim=1))\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "# Define Seq2Seq\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        trg_len = trg.shape[0]\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        encoder_outputs, hidden, cell = self.encoder(src)\n",
    "        input = trg[0, :]\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell, encoder_outputs)\n",
    "            outputs[t] = output\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[t] if random.random() < teacher_forcing_ratio else top1\n",
    "        return outputs\n",
    "\n",
    "# Initialize model\n",
    "INPUT_DIM = len(vocab)\n",
    "OUTPUT_DIM = len(vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "attn = BahdanauAttention(HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT, attn)\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "# Initialize optimizer and criterion\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "TRG_PAD_IDX = vocab['<pad>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)\n",
    "\n",
    "# Training function\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in iterator:\n",
    "        src, trg = batch\n",
    "        src = torch.nn.utils.rnn.pad_sequence(src, padding_value=vocab['<pad>'])\n",
    "        trg = torch.nn.utils.rnn.pad_sequence(trg, padding_value=vocab['<pad>'])\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "# Validation function\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            src, trg = batch\n",
    "            src = torch.nn.utils.rnn.pad_sequence(src, padding_value=vocab['<pad>'])\n",
    "            trg = torch.nn.utils.rnn.pad_sequence(trg, padding_value=vocab['<pad>'])\n",
    "            output = model(src, trg, 0)\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "# Training the model\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, val_loader, criterion)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'seq2seq-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Val. Loss: {valid_loss:.3f}')\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('seq2seq-model.pt'))\n",
    "\n",
    "# Inference function\n",
    "def predict(model, question, vocab, max_len=50):\n",
    "    model.eval()\n",
    "    tokens = tokenize(question)\n",
    "    tokens = [vocab[\"<bos>\"]] + [vocab[token] for token in tokens] + [vocab[\"<eos>\"]]\n",
    "    src_tensor = torch.LongTensor(tokens).unsqueeze(1).to(device)\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden, cell = model.encoder(src_tensor)\n",
    "    trg_indexes = [vocab[\"<bos>\"]]\n",
    "    for i in range(max_len):\n",
    "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
    "        with torch.no_grad():\n",
    "            output, hidden, cell = model.decoder(trg_tensor, hidden, cell, encoder_outputs)\n",
    "        pred_token = output.argmax(1).item()\n",
    "        trg_indexes.append(pred_token)\n",
    "        if pred_token == vocab[\"<eos>\"]:\n",
    "            break\n",
    "    trg_tokens = [list(vocab.keys())[i] for i in trg_indexes]\n",
    "    return trg_tokens[1:-1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the AI Quiz Game!\n",
      "\n",
      "Question 1: What is AI?\n",
      "\n",
      "Player 1's turn:\n",
      "Player 1's Score for this question: 0\n",
      "\n",
      "Player 2's turn:\n",
      "Player 2's Score for this question: 0\n",
      "\n",
      "Question 2: What is ML?\n",
      "\n",
      "Player 1's turn:\n",
      "Player 1's Score for this question: 93.32372523065641\n",
      "\n",
      "Player 2's turn:\n",
      "Player 2's Score for this question: 43.3237252306564\n",
      "\n",
      "--- Final Scores ---\n",
      "Player 1: 93.32372523065641\n",
      "Player 2: 43.3237252306564\n",
      "Player 1 wins!\n"
     ]
    }
   ],
   "source": [
    "# Test the model interactively\n",
    "def interactive_test(model, questions, vocab):\n",
    "    print(\"Select a question to answer:\")\n",
    "    for i, q in enumerate(questions):\n",
    "        print(f\"{i + 1}. {q}\")\n",
    "    choice = int(input(\"Enter the number of the question: \")) - 1\n",
    "    user_input = input(\"Enter your answer: \")\n",
    "    prediction = predict(model, questions[choice], vocab)\n",
    "    print(f\"Model Prediction: {' '.join(prediction)}\")\n",
    "    print(f\"Your Answer: {user_input}\")\n",
    "\n",
    "# List of questions from the dataset\n",
    "questions = df['question'].unique()\n",
    "\n",
    "# Run interactive test\n",
    "interactive_test(model, questions, vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
