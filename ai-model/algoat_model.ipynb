{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('it_problems.csv')\n",
    "\n",
    "# Tokenize the problem descriptions\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(df['problem_description'])\n",
    "sequences = tokenizer.texts_to_sequences(df['problem_description'])\n",
    "\n",
    "# Pad the sequences to ensure uniform input length\n",
    "x_data = keras.preprocessing.sequence.pad_sequences(sequences, maxlen=200)\n",
    "\n",
    "# Convert the solutions to categorical data\n",
    "y_data = pd.get_dummies(df['solution']).values\n",
    "\n",
    "# Build the model\n",
    "model = keras.Sequential([\n",
    "    layers.Embedding(input_dim=10000, output_dim=64, input_length=200),\n",
    "    layers.LSTM(64),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(y_data.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_data, y_data, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(x_data, y_data)\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "# Make predictions\n",
    "new_problems = [\"Example problem description\"]\n",
    "new_sequences = tokenizer.texts_to_sequences(new_problems)\n",
    "new_padded = keras.preprocessing.sequence.pad_sequences(new_sequences, maxlen=200)\n",
    "\n",
    "predictions = model.predict(new_padded)\n",
    "predicted_solution = np.argmax(predictions, axis=1)\n",
    "\n",
    "print(predicted_solution)\n",
    "\n",
    "# Save the model\n",
    "model.save('it_problem_solver_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'it_problems_with_exploded_answers_and_important_words.csv' has been created.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the data for the dataset\n",
    "data = {\n",
    "    'problem_description': [\n",
    "        \"How to fix a '404 Not Found' error?\",\n",
    "        \"How to recover a lost password on Windows?\",\n",
    "        \"How to install Python on Linux?\",\n",
    "        \"How to set up a virtual environment in Python?\",\n",
    "        \"How to resolve a 'Connection refused' error in a web server?\"\n",
    "    ],\n",
    "    'correct_answers': [\n",
    "        \"Check if the URL is correct or if the page has been moved.|Ensure the server is running and the path is correct.|Clear the browser cache and refresh.\",\n",
    "        \"Use the password reset feature on the login screen.|Follow the steps provided in the recovery email.|Use the Windows security questions if set up.\",\n",
    "        \"Use the package manager to install Python, e.g., `sudo apt-get install python3`.|Download and install from the official Python website.|Use `conda install python` if using Anaconda.\",\n",
    "        \"Use `venv` or `virtualenv` to create an isolated environment.|Run `python -m venv myenv` to set up the environment.|Activate the environment with `source myenv/bin/activate`.\",\n",
    "        \"Check if the web server is running and if the firewall settings are correct.|Verify that the server is listening on the correct port.|Check server logs for errors.\"\n",
    "    ],\n",
    "    'important_words': [\n",
    "        \"404,Not Found,error\",\n",
    "        \"recover,lost,password,Windows\",\n",
    "        \"install,Python,Linux\",\n",
    "        \"set up,virtual environment,Python\",\n",
    "        \"resolve,Connection refused,web server\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Explode the 'correct_answers' column\n",
    "df_correct_answers = df.set_index('problem_description')['correct_answers'].str.split('|', expand=True).stack().reset_index(level=1, drop=True).reset_index(name='correct_answer')\n",
    "\n",
    "# Explode the 'important_words' column\n",
    "df_important_words = df.set_index('problem_description')['important_words'].str.split(',', expand=True).stack().reset_index(level=1, drop=True).reset_index(name='important_words')\n",
    "\n",
    "# Merge the exploded DataFrames\n",
    "df_exploded = df_correct_answers.merge(df_important_words, on='problem_description')\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_exploded.to_csv('it_problems_with_exploded_answers_and_important_words.csv', index=False)\n",
    "print(\"CSV file 'it_problems_with_exploded_answers_and_important_words.csv' has been created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.04s/it]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"d:\\Software\\environments\\deep_learning\\environments\\deep_learning\\lib\\site-packages\\keras\\saving\\saving_utils.py\", line 147, in _wrapped_model  *\n        outputs = model(*args, **kwargs)\n    File \"d:\\Software\\environments\\deep_learning\\environments\\deep_learning\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\__autograph_generated_filev0pfdyx4.py\", line 26, in tf__call\n        ag__.if_stmt(ag__.ld(important_weights) is not None, if_body, else_body, get_state, set_state, ('x',), 1)\n    File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\__autograph_generated_filev0pfdyx4.py\", line 21, in if_body\n        x = ag__.ld(x) * ag__.ld(important_weights)\n\n    TypeError: Exception encountered when calling layer \"attention_model_1\" \"                 f\"(type AttentionModel).\n    \n    in user code:\n    \n        File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_19176\\1344629428.py\", line 61, in call  *\n            x = x * important_weights\n    \n        TypeError: Input 'y' of 'Mul' Op has type float64 that does not match type float32 of argument 'x'.\n    \n    \n    Call arguments received by layer \"attention_model_1\" \"                 f\"(type AttentionModel):\n      • x=tf.Tensor(shape=(None, 100), dtype=int32)\n      • important_weights=tf.Tensor(shape=(None, 100, 64), dtype=float64)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 109\u001b[0m\n\u001b[0;32m    106\u001b[0m         model\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(gradients, model\u001b[38;5;241m.\u001b[39mtrainable_variables))\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# Save the model in the SavedModel format\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msaved_attention_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaved_attention_model\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_embeddings\u001b[39m(sequences, important_words_indices):\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;66;03m# Tokenize and pad the sequences\u001b[39;00m\n",
      "File \u001b[1;32md:\\Software\\environments\\deep_learning\\environments\\deep_learning\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileza9g2fjz.py:14\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf___wrapped_model\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     12\u001b[0m (args, kwargs) \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(model)\u001b[38;5;241m.\u001b[39m_call_spec\u001b[38;5;241m.\u001b[39mset_arg_value, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m, ag__\u001b[38;5;241m.\u001b[39mld(args), ag__\u001b[38;5;241m.\u001b[39mld(kwargs)), \u001b[38;5;28mdict\u001b[39m(inputs_in_args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), fscope)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(base_layer_utils)\u001b[38;5;241m.\u001b[39mcall_context()\u001b[38;5;241m.\u001b[39menter(ag__\u001b[38;5;241m.\u001b[39mld(model), inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, build_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, saving\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m---> 14\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m output_names \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(model)\u001b[38;5;241m.\u001b[39moutput_names\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state\u001b[39m():\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filev0pfdyx4.py:26\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, x, important_weights)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mnonlocal\u001b[39;00m x\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mif_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimportant_weights\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melse_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m (lstm_out, forward_h, forward_c, backward_h, backward_c) \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mlstm, (ag__\u001b[38;5;241m.\u001b[39mld(x),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     28\u001b[0m state_h \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mconcat, ([ag__\u001b[38;5;241m.\u001b[39mld(forward_h), ag__\u001b[38;5;241m.\u001b[39mld(backward_h)],), \u001b[38;5;28mdict\u001b[39m(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), fscope)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filev0pfdyx4.py:21\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.if_body\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mif_body\u001b[39m():\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mnonlocal\u001b[39;00m x\n\u001b[1;32m---> 21\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimportant_weights\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: in user code:\n\n    File \"d:\\Software\\environments\\deep_learning\\environments\\deep_learning\\lib\\site-packages\\keras\\saving\\saving_utils.py\", line 147, in _wrapped_model  *\n        outputs = model(*args, **kwargs)\n    File \"d:\\Software\\environments\\deep_learning\\environments\\deep_learning\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\__autograph_generated_filev0pfdyx4.py\", line 26, in tf__call\n        ag__.if_stmt(ag__.ld(important_weights) is not None, if_body, else_body, get_state, set_state, ('x',), 1)\n    File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\__autograph_generated_filev0pfdyx4.py\", line 21, in if_body\n        x = ag__.ld(x) * ag__.ld(important_weights)\n\n    TypeError: Exception encountered when calling layer \"attention_model_1\" \"                 f\"(type AttentionModel).\n    \n    in user code:\n    \n        File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_19176\\1344629428.py\", line 61, in call  *\n            x = x * important_weights\n    \n        TypeError: Input 'y' of 'Mul' Op has type float64 that does not match type float32 of argument 'x'.\n    \n    \n    Call arguments received by layer \"attention_model_1\" \"                 f\"(type AttentionModel):\n      • x=tf.Tensor(shape=(None, 100), dtype=int32)\n      • important_weights=tf.Tensor(shape=(None, 100, 64), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('it_problems_with_exploded_answers_and_important_words.csv')\n",
    "\n",
    "# Combine all texts (questions, answers, and important words) for tokenization\n",
    "all_texts = df['problem_description'].tolist() + df['correct_answer'].tolist() + df['important_words'].str.split(',').explode().tolist()\n",
    "\n",
    "# Tokenize and pad the texts\n",
    "max_length = 100\n",
    "vocab_size = 10000\n",
    "embedding_dim = 64\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(all_texts)\n",
    "sequences = tokenizer.texts_to_sequences(all_texts)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Split the sequences back into questions, answers, and important words\n",
    "question_sequences = padded_sequences[:len(df)]\n",
    "answer_sequences = padded_sequences[len(df):len(df) * 2]\n",
    "important_words_sequences = padded_sequences[len(df) * 2:]\n",
    "\n",
    "# Define the Bahdanau Attention layer\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "    \n",
    "    def call(self, query, values):\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "# Define the model with attention and dynamic embedding adjustment\n",
    "class AttentionModel(Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, units):\n",
    "        super(AttentionModel, self).__init__()\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = Bidirectional(LSTM(units, return_sequences=True, return_state=True))\n",
    "        self.attention = BahdanauAttention(units)\n",
    "        self.dense = Dense(units, activation='relu')\n",
    "    \n",
    "    def call(self, x, important_weights=None):\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        if important_weights is not None:\n",
    "            x = x * important_weights\n",
    "\n",
    "        lstm_out, forward_h, forward_c, backward_h, backward_c = self.lstm(x)\n",
    "        state_h = tf.concat([forward_h, backward_h], axis=1)\n",
    "        context_vector, attention_weights = self.attention(state_h, lstm_out)\n",
    "        output = self.dense(context_vector)\n",
    "        return output\n",
    "\n",
    "units = 64\n",
    "model = AttentionModel(vocab_size, embedding_dim, units)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Prepare the important words indices for each question-answer pair\n",
    "important_words_indices = []\n",
    "for words in df['important_words']:\n",
    "    indices = [tokenizer.word_index.get(word) for word in words.split(',') if tokenizer.word_index.get(word) is not None]\n",
    "    important_words_indices.append(indices)\n",
    "\n",
    "# Train the model using the real target embeddings and important words\n",
    "answer_embeddings = model(answer_sequences).numpy()\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch + 1}/{epochs}')\n",
    "    with tqdm(total=len(question_sequences), desc='Training') as pbar:\n",
    "        for i in range(len(question_sequences)):\n",
    "            important_indices = np.array(important_words_indices[i:i+1], dtype=object)\n",
    "            with tf.GradientTape() as tape:\n",
    "                outputs = model(question_sequences[i:i+1], important_weights=None)\n",
    "                loss = tf.keras.losses.mean_squared_error(answer_embeddings[i:i+1], outputs)\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            pbar.update(1)\n",
    "\n",
    "# Save the model in the SavedModel format\n",
    "model.save('saved_attention_model', save_format='tf')\n",
    "print(\"Model saved as 'saved_attention_model'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(sequences, important_words_indices):\n",
    "    # Tokenize and pad the sequences\n",
    "    sequences = tokenizer.texts_to_sequences(sequences)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "    # Ensure the shapes are compatible for tensor_scatter_nd_update\n",
    "    batch_size = padded_sequences.shape[0]\n",
    "    seq_length = padded_sequences.shape[1]\n",
    "    hidden_dim = 64  # Example hidden dimension size; adjust as necessary\n",
    "\n",
    "    # Create an initial tensor of ones with the desired shape\n",
    "    important_weights = tf.ones((batch_size, seq_length, hidden_dim))\n",
    "\n",
    "    # Update the tensor based on important words indices\n",
    "    for i, indices in enumerate(important_words_indices):\n",
    "        if len(indices) > 0:\n",
    "            indices = np.array(indices)\n",
    "            indices = indices[indices < seq_length]\n",
    "            updates = tf.ones((len(indices), hidden_dim)) * 2.0\n",
    "            updates = tf.reshape(updates, (len(indices), hidden_dim))\n",
    "            important_weights = tf.tensor_scatter_nd_update(important_weights, tf.constant([[i, idx] for idx in indices], dtype=tf.int32), updates)\n",
    "\n",
    "    # Perform the forward pass\n",
    "    embeddings = model(padded_sequences, important_weights=important_weights).numpy()\n",
    "    return embeddings\n",
    "\n",
    "def find_most_similar_answer(answer, correct_answers, important_words):\n",
    "    answers_to_compare = correct_answers + [answer]\n",
    "    important_words_indices = [\n",
    "        [tokenizer.word_index[word] for word in answer.split() if word in important_words and word in tokenizer.word_index] \n",
    "        for answer in answers_to_compare\n",
    "    ]\n",
    "    embeddings = get_embeddings(answers_to_compare, important_words_indices)\n",
    "    similarities = cosine_similarity([embeddings[-1]], embeddings[:-1])[0]\n",
    "    most_similar_index = np.argmax(similarities)\n",
    "    return most_similar_index, similarities[most_similar_index]\n",
    "\n",
    "def ask_question(question, correct_answers, important_words):\n",
    "    print(f\"Question: {question}\")\n",
    "    answer1 = input(\"Player 1, your answer: \")\n",
    "    answer2 = input(\"Player 2, your answer: \")\n",
    "    index1, similarity1 = find_most_similar_answer(answer1, correct_answers, important_words)\n",
    "    index2, similarity2 = find_most_similar_answer(answer2, correct_answers, important_words)\n",
    "    return similarity1, similarity2, index1, index2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_menu():\n",
    "    print(\"Welcome to the IT Problem-Solving Game!\")\n",
    "    print(\"1. Start Game\")\n",
    "    print(\"2. Exit\")\n",
    "    choice = input(\"Enter your choice: \")\n",
    "    return choice\n",
    "\n",
    "def main():\n",
    "    while True:\n",
    "        choice = display_menu()\n",
    "        if choice == '1':\n",
    "            question_row = df.sample(n=1).iloc[0]\n",
    "            question = question_row['problem_description']\n",
    "            correct_answers = df[df['problem_description'] == question]['correct_answer'].tolist()\n",
    "            important_words = df[df['problem_description'] == question]['important_words'].tolist()\n",
    "            similarity1, similarity2, index1, index2 = ask_question(question, correct_answers, important_words)\n",
    "            if similarity1 > similarity2:\n",
    "                print(f\"Player 1 wins![{similarity1}|{similarity2}]\")\n",
    "            elif similarity2 > similarity1:\n",
    "                print(f\"Player 2 wins![{similarity1}|{similarity2}]\")\n",
    "            else:\n",
    "                print(f\"TIE! [{similarity1}|{similarity2}]\")\n",
    "        elif choice == '2':\n",
    "            print(\"Thanks for playing!\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid choice. Please try again.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
