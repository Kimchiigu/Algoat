{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import random\n",
    "\n",
    "# Load dataset\n",
    "def load_dataset(file_path):\n",
    "    dataset = tf.data.TextLineDataset(file_path)\n",
    "    dataset = dataset.map(lambda x: tf.strings.split(x, '\\t'))\n",
    "    dataset = dataset.map(lambda x: {'question': x[0], 'answer1': x[1], 'answer2': x[2], 'label': tf.strings.to_number(x[3], tf.int32)})\n",
    "    return dataset\n",
    "\n",
    "# Tokenize dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/gemma-2b')\n",
    "\n",
    "def tokenize_function(example):\n",
    "    question = tokenizer(example['question'].numpy().decode('utf-8'), truncation=True, padding='max_length', max_length=128)\n",
    "    answer1 = tokenizer(example['answer1'].numpy().decode('utf-8'), truncation=True, padding='max_length', max_length=256)\n",
    "    answer2 = tokenizer(example['answer2'].numpy().decode('utf-8'), truncation=True, padding='max_length', max_length=256)\n",
    "    return {\n",
    "        'input_ids_question': question['input_ids'],\n",
    "        'attention_mask_question': question['attention_mask'],\n",
    "        'input_ids_answer1': answer1['input_ids'],\n",
    "        'attention_mask_answer1': answer1['attention_mask'],\n",
    "        'input_ids_answer2': answer2['input_ids'],\n",
    "        'attention_mask_answer2': answer2['attention_mask'],\n",
    "        'label': example['label'],\n",
    "    }\n",
    "\n",
    "def tf_tokenize_function(example):\n",
    "    result = tf.py_function(tokenize_function, [example], \n",
    "                          {'input_ids_question': tf.int32, 'attention_mask_question': tf.int32, \n",
    "                           'input_ids_answer1': tf.int32, 'attention_mask_answer1': tf.int32, \n",
    "                           'input_ids_answer2': tf.int32, 'attention_mask_answer2': tf.int32, \n",
    "                           'label': tf.int32})\n",
    "    result['input_ids_question'].set_shape([128])\n",
    "    result['attention_mask_question'].set_shape([128])\n",
    "    result['input_ids_answer1'].set_shape([256])\n",
    "    result['attention_mask_answer1'].set_shape([256])\n",
    "    result['input_ids_answer2'].set_shape([256])\n",
    "    result['attention_mask_answer2'].set_shape([256])\n",
    "    return result\n",
    "\n",
    "# Create model\n",
    "class ComparisonModel(tf.keras.Model):\n",
    "    def __init__(self, base_model_name):\n",
    "        super(ComparisonModel, self).__init__()\n",
    "        self.base_model = TFAutoModel.from_pretrained(base_model_name)\n",
    "        self.dense = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        question_inputs = {'input_ids': inputs['input_ids_question'], 'attention_mask': inputs['attention_mask_question']}\n",
    "        answer1_inputs = {'input_ids': inputs['input_ids_answer1'], 'attention_mask': inputs['attention_mask_answer1']}\n",
    "        answer2_inputs = {'input_ids': inputs['input_ids_answer2'], 'attention_mask': inputs['attention_mask_answer2']}\n",
    "\n",
    "        question_outputs = self.base_model(**question_inputs)[0]\n",
    "        answer1_outputs = self.base_model(**answer1_inputs)[0]\n",
    "        answer2_outputs = self.base_model(**answer2_inputs)[0]\n",
    "\n",
    "        # Pool the outputs\n",
    "        question_embedding = tf.reduce_mean(question_outputs, axis=1)\n",
    "        answer1_embedding = tf.reduce_mean(answer1_outputs, axis=1)\n",
    "        answer2_embedding = tf.reduce_mean(answer2_outputs, axis=1)\n",
    "\n",
    "        # Concatenate embeddings\n",
    "        combined = tf.concat([question_embedding, answer1_embedding, answer2_embedding], axis=-1)\n",
    "        output = self.dense(combined)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Instantiate and compile model\n",
    "model = ComparisonModel('google/gemma-2b')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "metrics = [tf.keras.metrics.BinaryAccuracy()]\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "# Load and tokenize dataset for training\n",
    "file_path = 'edukasi_it_qa_comparison.txt'\n",
    "dataset = load_dataset(file_path)\n",
    "dataset = dataset.map(tf_tokenize_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "dataset = dataset.batch(32)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Train model\n",
    "model.fit(dataset, epochs=10, validation_steps=5)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
